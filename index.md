# ROAR XD

This website describes the work that Alfredo, Alvin, Aman, Sihao, and Wesley did for their EECS 206A final project at UC Berkeley during Fall 2020.

## Introduction

For our final project, we participated in [Berkeley’s ROAR competition](https://vivecenter.berkeley.edu/research1/roar/), in which teams raced 1/10th scale self-driving cars around a track and competed for various prizes. Due to the pandemic, for 2020 this competition was done through a Carla simulation, and each team was provided with the same [starter code](https://github.com/augcog/ROAR) to build off of. We were fortunate enough to win the Grand Prize for this year’s *2020 ROAR S1 series* and have published [our code](https://github.com/asdegoyeneche/ROAR) in all of its messy glory on GitHub. 

However, the end goal of our project was not only to win a race, but also to create a simplified model of an autonomous mobility-as-a-service system (think Uber or Lyft with self-driving vehicles, or your Tesla driving people around while you work an office job). As such, in addition to driving at high speeds, we were interested in following lanes and traffic signals, detecting and avoiding obstacles in our path, and providing a smooth and stable ride, which made our project more interesting than just a racing project.


## Design

We worked in a world where the car knew its current speed, position, and orientation in a *world frame* (either via speedometer, GPS, and compass or through Carla magic) and was controlled by a steering wheel and a gas pedal (known in Carla as *steering* and *throttle*, respectively). There was also a brake pedal, but that was more for decoration than anything. The car was provided with a list of *waypoints* (with coordinates in the world frame) that traveled along the desired route. In our project, these waypoints were provided in a text file, but can also be generated by something like Google Maps in the real world. There were lane lines on the road, barriers along the sides of the road, and an occasional stationary car or barrier in the middle of the road to keep things interesting.

Given this setup, our goals were to detect lanes and obstacles, plan a path that roughly follows the given waypoints while staying in our lane and avoiding obstacles, and control the car so that it drives smoothly and accurately along our planned path. For the race, we didn’t worry about lanes and obstacles and instead modified our waypoints and controlled the car so that it went as fast as possible with minimal crashing.

With these goals in mind, we split these tasks into four parts -- sensing, planning, controlling, and racing -- and made the following design choices:

### Sensing and Planning

Only using pre-recorded waypoints, the controller in the starter code is able to run a loop without the assistance of perception. However, there are some zig-zags in the path due to the poor quality of the waypoints. In order to overcome this, we optimize the path by letting the car follow the lane center and avoid objects. We use a front RGB camera to detect white and yellow lane segments in front of the car and use a front depth camera to calculate their 3-D coordinates. Instead of only following lanes, which sometimes cannot be detected, we use lanes as an augmentation to “correct” improper waypoints. That is, the input of the controller is decided by a combination of lane center and pre-recorded waypoints. This design allows us to generate a smooth path when lanes are visible and follow the waypoints when lanes are not visible.

### Controlling

The controller receives the coordinates of the next waypoint and the desired speed from the path planner. From this, we found the *difference in speed* between our current speed and target speed and the *difference in angle* between our current trajectory and the next waypoint (see figure below), and selected steering and throttle values to correct any deviation.

[figure goes here]

The starter code came with two separate [PID controllers](https://en.wikipedia.org/wiki/PID_controller), but we found this difficult to tune due to the mental mismatch between the quantitative $K_p, K_i, K_d$ parameters and the qualitative desired behavior (e.g. “more sensitive steering”). Additionally, there was no coordination between steering and throttle, which resulted in very aggressive turns, and the PID controllers did not account for the drag of the car, which resulted in the car going ~10% slower than the target speed.

To correct these issues, we made two notable changes. First, we added reactive speed control, which reduced the target speed when there was a large error in direction. This allowed us to slow down as we turned and resulted in smoother turns and better recovery when off-track with a small trade-off in speed. Second, we designed and implemented an [LQR controller](https://en.wikipedia.org/wiki/Linear%E2%80%93quadratic_regulator) to use instead of the PID controllers. This enabled the controller to account for the car’s dynamics (e.g. drag) when selecting steering and throttle values, which resulted in more accurate adherence to the planned path. The parameters used in the LQR were also more intuitive to tune than the parameters in the PID. For instance, if the car was veering back and forth from over-steering, we could increase the *cost* of steering in the LQR controller, and the controller would steer less aggressively. This ease of tuning allowed us to get significantly better performance and reach higher speeds when compared to the PID controllers. However, although the LQR had better performance than the PID, it was also significantly more complex. We had to spend a lot of time determining the system dynamics of the car and finagling the LQR math to converge to a non-zero desired trajectory. The grimy details of the LQR controller can be found in the [Implementation section](???can we place an anchor here???).

### Racing


## Implementation

### Sensing

#### Lane Detection

![Lane Detection](images/lane_detection.png)

The lane detector takes the images captured by the front RGB and depth camera as input and calculates the 3-D world coornidates of the left lane, right lane, and lane center in sight. At each time step, the procedure of lane detection algorithm is as follows:

- Converts the original RGB image to a grayscale image.
- Calculates the canny edges of the image.
- Cuts out two triangle-shaped areas of interests, one on the left bottom part of the image, another on the right bottom part.
- Calculates hough lines from each image and take their average to form two straight lines as our detected lanes.
- Calculates world coordinates of lanes using depth camera image.
- Use the average of world coordinates of left and right lanes as the lane center.

### Planning

`lane_following_local_planner.py` contains the main logic of our lane following planner. We define a variable, confidence, which is a scalar between 0 and 1. It decays exponentially when a lane is not detected, and is reset to 1 when both are detected.  We calculate the target location based on a weighted sum of the mid point of lane center and the next waypoint. We also limit the target speed by confidence. The location of the next waypoint and target speed are then fed into our controller. The pseudocode of the lane following algorithm is as follows:

```python3
def follow_lane(waypoints, α: confidence decay rate 0 ~ 1, β: speed limit factor > 0):
    confidence = 1
    for each step:
        left_lane, right_lane, lane_center = detect_lanes_from_camera_input()
        if left_lane or right_lane not detected:
            confidence = confidence * α
        else:
            confidence = 1
        target_location = mid_point(lane_center) * confidence + 
                              get_next_waypoint(waypoints) * (1 - confidence)
        target_speed = maximum_speed * exp((1 - confidence) * -β)
        controller.run(target_location, target_speed)
```

### Controlling


## Results

Now we get to show our fun little videos of the car driving!

### Sensing and Planning

Our car successfully detects and follows lanes at lower speeds (50 km/hr)! Below are two instances of the car driving. The one above directly follows the waypoints, while the one below includes lane-keeping.

{% include youtubePlayer.html id="dbFgMa_l4K8" %}

{% include youtubePlayer.html id="rZt_j8n-sSU" %}

We found that it was difficult to keep track of lanes denoted by dashed lines because the dashes were short and spaced rather far apart. This problem was exacerbated during turns, where the dashed lines in a lane were not even aligned with each other, making it difficult to stay in our lane while turning at higher speeds.

We were also able to detect obstacles in front of the car, as seen below, but did not have enough time to incorporate it into our planning for obstacle avoidance.

[video or image of car detecting obstacles?]

### Controlling
The LQR controller performed notably better than the original PID controllers. Below we can see two instances of the car. The one on the left is using the PID controllers, while the one on the right is using the LQR controller. We tuned both controllers as well as we could.

[PID car]  [LQR car]

### Racing

The waypoint-lookahead planner allowed us to take turns more smoothly and cut in on the corners. Below are two instances of the car taking a turn. The one on the left is without lookahead, while the one on the right uses waypoint lookahead.

[squiggly turn car]  [smooth turn car]

We found that the performance of the lookahead planner was dependent on the speed of the car, the placement of the waypoints, and the shape of the turn. Thus, we do not know how well this planner will perform on an arbitrary track that we haven’t seen before. However, as seen above, given the opportunity to tune the speed of the car and the amount we look ahead, we can achieve very good turns.

We submitted our car into the ROAR competition and won the Grand Prize for this year’s 2020 ROAR S1 series, reaching a max speed of 188 km/hr along a figure-eight shaped track. A video of our car running a single lap can be found below. There are some tight corners and some instances where the car slows down unnecessarily, but overall the racing portion performed quite well.

[video of car running a single lap]


## Conclusion

We successfully achieved most of our goals for this project! Our lane detection and obstacle detection are functional and we have implemented lane-keeping in our path planner. The LQR controller provides us with a smooth ride and our waypoint-lookahead planner is very suitable for racing at high speeds. It is unfortunate that we were unable to implement obstacle avoidance, but given our current work, our car can pick up a rider and drive to a desired drop-off location while following lanes! Granted, it will be running through red lights and demolishing pedestrians, but that just means it works best at, like, 3 am!

I think one of our biggest difficulties was a lack of regular communication between our team members. Of course, there were design questions of what we wanted our project to do and how each person’s work fit into the whole, and there were technical challenges of how to actually implement what we want, but I feel that much of that could have been resolved more efficiently through a brainstorming session, or having two or three people troubleshooting a problem, or catching up more frequently on what each person has done and how that affects the parts of the project that other people are working on. Instead, after distributing tasks and submitting our proposal, our group didn’t meet up again (or even talk to each other) for the next five weeks. During that time, a couple of us started working on our tasks, got stuck, and gave up without asking the rest of the group for help. Other group members were busy with other coursework and research, and did not make any notable progress. By the time we finally met up again, there was only one and a half weeks left before the presentation and ROAR competition, and we only had two partially-functional components. After this, we started meeting more frequently, posting our progress and updates in our group chat, and brainstorming and troubleshooting with each other as necessary, which let us make tremendous progress during that last week and create a project worth presenting on and competing with. 


## Team

**Alfredo De Goyeneche.** 

**Alvin Tan.** Alvin is a first-year electrical engineering PhD student at UC Berkeley. He graduated from Northwestern University in 2020 with a BS in computer engineering, economics, and math, and is currently researching wireless sensor networks under Prof. Dutta in Lab11. For this project, he designed and implemented the LQR controller, helped other group members troubleshoot their work, and coordinated the progress of the overall project. 

**Aman Sidhant.** 

**Sihao Chen.** Sihao is a Master of Engineering student in UC Berkeley EECS. He graduated from Northeastern University (China) in 2020 with a B.Eng. in software engineering. He is in the assistive mouse capstone project group supervised by Prof. Brian A. Barsky.  He led the perception part of this project, designed and implemented the lane detector and the lane following planner, and assisted with object detection.

**Wesley Wang.** 


## Additional materials
